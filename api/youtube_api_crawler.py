"""
YouTube Data API v3ë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ê¸‰ìƒìŠ¹ Shorts í¬ë¡¤ëŸ¬
"""
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import os
from dotenv import load_dotenv

load_dotenv()

class YouTubeAPIShortsCrawler:
    def __init__(self, cache_file: str = "../data/youtube_shorts_cache.json"):
        self.cache_file = Path(cache_file)
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)
        self.api_key = os.getenv('YOUTUBE_API_KEY', '')
        
    def get_trending_shorts(self, max_results: int = 50) -> List[Dict]:
        """YouTube Data APIë¡œ ì‹¤ì œ ê¸‰ìƒìŠ¹ Shorts ê°€ì ¸ì˜¤ê¸°"""
        
        if not self.api_key:
            print("âš ï¸ YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            print("   ì‹¤ì œ ê¸‰ìƒìŠ¹ ì˜ìƒì„ ê°€ì ¸ì˜¤ë ¤ë©´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            print("   YOUTUBE_API_SETUP.md íŒŒì¼ ì°¸ê³ ")
            return self._get_fallback_data()
        
        try:
            print(f"ğŸ¬ YouTube Data APIë¡œ ì‹¤ì œ ê¸‰ìƒìŠ¹ Shorts ìˆ˜ì§‘ ì¤‘...")
            
            youtube = build('youtube', 'v3', developerKey=self.api_key)
            
            videos = []
            
            # 1. í•œêµ­ ê¸‰ìƒìŠ¹ ë™ì˜ìƒ ê°€ì ¸ì˜¤ê¸° (mostPopular chart)
            videos_response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                chart='mostPopular',  # ê¸‰ìƒìŠ¹ ì°¨íŠ¸
                regionCode='KR',  # í•œêµ­
                videoCategoryId='0',  # ëª¨ë“  ì¹´í…Œê³ ë¦¬
                maxResults=min(max_results, 50)
            ).execute()
            
            for item in videos_response.get('items', []):
                # Shorts ì˜ìƒë§Œ í•„í„°ë§ (60ì´ˆ ì´í•˜)
                duration = item.get('contentDetails', {}).get('duration', '')
                if self._is_short_duration(duration):
                    video_info = self._parse_video_data(item)
                    if video_info:
                        videos.append(video_info)
            
            # Shortsê°€ ë¶€ì¡±í•˜ë©´ Shorts íƒœê·¸ë¡œ ê²€ìƒ‰
            if len(videos) < 20:
                search_response = youtube.search().list(
                    part='snippet',
                    q='#Shorts',
                    regionCode='KR',
                    type='video',
                    order='viewCount',
                    maxResults=30
                ).execute()
                
                video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                if video_ids:
                    search_videos_response = youtube.videos().list(
                        part='snippet,statistics,contentDetails',
                        id=','.join(video_ids)
                    ).execute()
                    
                    for item in search_videos_response.get('items', []):
                        if self._is_short_duration(item.get('contentDetails', {}).get('duration', '')):
                            video_info = self._parse_video_data(item)
                            if video_info and video_info not in videos:
                                videos.append(video_info)
            
            print(f"âœ… ì‹¤ì œ ê¸‰ìƒìŠ¹ Shorts {len(videos)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
            return videos[:max_results]
            
        except HttpError as e:
            print(f"âŒ YouTube API ì˜¤ë¥˜: {e}")
            print("   API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” API í‚¤ ë¬¸ì œ")
            return self._get_fallback_data()
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return self._get_fallback_data()
    
    def _is_short_duration(self, duration: str) -> bool:
        """ì˜ìƒì´ 60ì´ˆ ì´í•˜ì¸ì§€ í™•ì¸ (Shorts ê¸°ì¤€)"""
        import re
        
        # ISO 8601 duration í˜•ì‹ íŒŒì‹± (ì˜ˆ: PT1M30S = 1ë¶„ 30ì´ˆ)
        match = re.match(r'PT(?:(\d+)M)?(?:(\d+)S)?', duration)
        if match:
            minutes = int(match.group(1) or 0)
            seconds = int(match.group(2) or 0)
            total_seconds = minutes * 60 + seconds
            return total_seconds <= 60
        return False
    
    def _parse_video_data(self, item: dict) -> Dict:
        """YouTube API ì‘ë‹µì„ íŒŒì‹±"""
        try:
            snippet = item.get('snippet', {})
            statistics = item.get('statistics', {})
            
            title = snippet.get('title', '')
            video_id = item.get('id', '')
            category_id = snippet.get('categoryId', '')
            
            # ì¡°íšŒìˆ˜
            view_count = int(statistics.get('viewCount', 0))
            views_formatted = self._format_view_count(view_count)
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category = self._map_category(category_id, title)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(title)
            
            return {
                "title": title,
                "category": category,
                "views": views_formatted,
                "engagement": self._calculate_engagement(view_count),
                "keywords": keywords,
                "thumbnail": self._get_emoji(category),
                "why_viral": self._analyze_viral(title, view_count),
                "video_id": video_id,
                "youtube_url": f"https://www.youtube.com/watch?v={video_id}",
                "shorts_url": f"https://www.youtube.com/shorts/{video_id}",
                "is_shorts": True,
                "region": "êµ­ë‚´",
                "language": "í•œêµ­ì–´",
                "trend_score": self._calculate_trend_score(view_count),
                "crawled_at": datetime.now().isoformat()
            }
        except Exception as e:
            print(f"íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _format_view_count(self, count: int) -> str:
        """ì¡°íšŒìˆ˜ í¬ë§·íŒ…"""
        if count >= 1000000:
            return f"{count/1000000:.1f}M"
        elif count >= 1000:
            return f"{count/1000:.0f}K"
        return str(count)
    
    def _calculate_engagement(self, view_count: int) -> str:
        """ì°¸ì—¬ë„ ê³„ì‚°"""
        if view_count >= 1000000:
            return "ë§¤ìš°ë†’ìŒ"
        elif view_count >= 100000:
            return "ë†’ìŒ"
        return "ë³´í†µ"
    
    def _calculate_trend_score(self, view_count: int) -> int:
        """íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°"""
        if view_count >= 5000000:
            return 100
        elif view_count >= 1000000:
            return 95
        elif view_count >= 500000:
            return 90
        elif view_count >= 100000:
            return 85
        return 70
    
    def _map_category(self, category_id: str, title: str) -> str:
        """YouTube ì¹´í…Œê³ ë¦¬ IDë¥¼ í•œê¸€ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘"""
        category_map = {
            '1': 'ì˜í™”/ì• ë‹ˆë©”ì´ì…˜',
            '2': 'ìë™ì°¨/ì°¨ëŸ‰',
            '10': 'ìŒì•…',
            '15': 'ë°˜ë ¤ë™ë¬¼/ë™ë¬¼',
            '17': 'ìŠ¤í¬ì¸ ',
            '19': 'ì—¬í–‰/ì´ë²¤íŠ¸',
            '20': 'ê²Œì„',
            '22': 'ì¸ë¬¼/ë¸”ë¡œê·¸',
            '23': 'ì½”ë¯¸ë””',
            '24': 'ì—”í„°í…Œì¸ë¨¼íŠ¸',
            '25': 'ë‰´ìŠ¤/ì •ì¹˜',
            '26': 'ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼',
            '27': 'êµìœ¡',
            '28': 'ê³¼í•™ê¸°ìˆ ',
            '29': 'ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™'
        }
        
        category = category_map.get(category_id, 'ì¼ë°˜')
        
        # ì œëª© ê¸°ë°˜ ì¬ë¶„ë¥˜
        if any(word in title for word in ['ë¶€ì—…', 'ì°½ì—…', 'ì‚¬ì—…']):
            return 'ì°½ì—…/ë¶€ì—…'
        elif any(word in title for word in ['ì¬í…Œí¬', 'íˆ¬ì', 'ì£¼ì‹', 'ì½”ì¸']):
            return 'ì¬í…Œí¬/ê¸ˆìœµ'
        elif any(word in title for word in ['ë§ˆì¼€íŒ…', 'SNS', 'ì¸ìŠ¤íƒ€']):
            return 'ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤'
        
        return category
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        keyword_patterns = {
            'ë¶€ì—…': r'ë¶€ì—…|ì‚¬ì´ë“œ|nì¡|íˆ¬ì¡',
            'ì¬í…Œí¬': r'ì¬í…Œí¬|ëˆ|ìˆ˜ìµ|ë²Œê¸°',
            'íˆ¬ì': r'íˆ¬ì|ì£¼ì‹|ë¶€ë™ì‚°|ì½”ì¸',
            'AI': r'AI|ChatGPT|ì¸ê³µì§€ëŠ¥',
            'ê°œë°œ': r'ê°œë°œ|ì½”ë”©|í”„ë¡œê·¸ë˜ë°',
            'ë§ˆì¼€íŒ…': r'ë§ˆì¼€íŒ…|SNS|ì¸ìŠ¤íƒ€',
            'ì°½ì—…': r'ì°½ì—…|ì‚¬ì—…|ìŠ¤íƒ€íŠ¸ì—…',
            'ìê¸°ê³„ë°œ': r'ìê¸°ê³„ë°œ|ë£¨í‹´|ìŠµê´€'
        }
        
        keywords = []
        for keyword, pattern in keyword_patterns.items():
            if re.search(pattern, title, re.IGNORECASE):
                keywords.append(keyword)
        
        return keywords[:5] if keywords else ['íŠ¸ë Œë“œ']
    
    def _get_emoji(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì´ëª¨ì§€"""
        emojis = {
            'ì°½ì—…/ë¶€ì—…': 'ğŸ’¼', 'ì¬í…Œí¬/ê¸ˆìœµ': 'ğŸ’°', 'ê³¼í•™ê¸°ìˆ ': 'ğŸ”¬',
            'ìê¸°ê³„ë°œ': 'ğŸ’ª', 'ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤': 'ğŸ“±', 'ê²Œì„': 'ğŸ®',
            'ìš”ë¦¬/ìŒì‹': 'ğŸ³', 'êµìœ¡': 'ğŸ“š', 'ìŒì•…': 'ğŸµ',
            'ì˜í™”/ì• ë‹ˆë©”ì´ì…˜': 'ğŸ¬', 'ìë™ì°¨/ì°¨ëŸ‰': 'ğŸš—',
            'ë°˜ë ¤ë™ë¬¼/ë™ë¬¼': 'ğŸ¾', 'ìŠ¤í¬ì¸ ': 'âš½', 'ì—¬í–‰/ì´ë²¤íŠ¸': 'âœˆï¸',
            'ì¸ë¬¼/ë¸”ë¡œê·¸': 'ğŸ‘¤', 'ì½”ë¯¸ë””': 'ğŸ˜‚', 'ì—”í„°í…Œì¸ë¨¼íŠ¸': 'ğŸ­'
        }
        return emojis.get(category, 'ğŸ“º')
    
    def _analyze_viral(self, title: str, view_count: int) -> str:
        """ë°”ì´ëŸ´ ìš”ì†Œ ë¶„ì„"""
        elements = []
        
        if view_count >= 1000000:
            elements.append("ì´ˆëŒ€ë°• ì¡°íšŒìˆ˜")
        elif view_count >= 500000:
            elements.append("ë†’ì€ ì¡°íšŒìˆ˜")
        
        import re
        if re.search(r'\d+ë§Œì›|\d+ì–µ', title):
            elements.append("êµ¬ì²´ì  ê¸ˆì•¡")
        if re.search(r'\d+ê°œì›”|\d+ì¼', title):
            elements.append("ê¸°ê°„ ëª…ì‹œ")
        if re.search(r'ë¹„ë²•|ê¿€íŒ|ë°©ë²•', title):
            elements.append("í•˜ìš°íˆ¬")
        
        return " + ".join(elements) if elements else "ì¸ê¸° ì½˜í…ì¸ "
    
    def _get_fallback_data(self) -> List[Dict]:
        """API ì‚¬ìš© ë¶ˆê°€ ì‹œ ê³ í’ˆì§ˆ ì°¸ê³  ë°ì´í„°"""
        return [
            {
                "title": "ğŸ’¡ ì°¸ê³ ìš©: ë¶€ì—…ìœ¼ë¡œ ì›” 500ë§Œì› ë²„ëŠ” ë²•",
                "category": "ì°½ì—…/ë¶€ì—…",
                "views": "2.1M",
                "engagement": "ë§¤ìš°ë†’ìŒ",
                "keywords": ["ë¶€ì—…", "ì›”ìˆ˜ìµ", "ì¬í…Œí¬"],
                "thumbnail": "ğŸ’¼",
                "why_viral": "ì°¸ê³ ìš© íŠ¸ë Œë“œ ë°ì´í„°",
                "video_id": "",
                "youtube_url": "",
                "shorts_url": "",
                "is_shorts": True,
                "region": "êµ­ë‚´",
                "language": "í•œêµ­ì–´",
                "trend_score": 95,
                "crawled_at": datetime.now().isoformat(),
                "note": "ì°¸ê³ ìš© ë°ì´í„° - ì‹¤ì œ ì˜ìƒ ë§í¬ ì—†ìŒ"
            }
        ]
    
    def save_to_cache(self, data: List[Dict]):
        """ìºì‹œ ì €ì¥"""
        cache_data = {
            "last_updated": datetime.now().isoformat(),
            "videos": data,
            "count": len(data),
            "source": "youtube_data_api_v3" if self.api_key else "fallback_data"
        }
        
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(data)}ê°œ ì˜ìƒ")

if __name__ == "__main__":
    crawler = YouTubeAPIShortsCrawler()
    videos = crawler.get_trending_shorts(25)
    
    print(f"\nìˆ˜ì§‘ëœ ì˜ìƒ {len(videos)}ê°œ:")
    for i, v in enumerate(videos[:5], 1):
        print(f"{i}. {v['title']}")
        print(f"   ì¡°íšŒìˆ˜: {v['views']} | URL: {v['youtube_url']}")

