"""
YouTube ì‹¤ì‹œê°„ í¬ë¡¤ëŸ¬ - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
ìƒ˜í”Œ/ë”ë¯¸/ì„ì‹œ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€
"""
import json
import time
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re

class YouTubeRealtimeCrawler:
    def __init__(self, cache_file: str = "../data/youtube_realtime_cache.json"):
        self.cache_file = Path(cache_file)
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)
        self.last_update = None
        self.update_interval = 2 * 60 * 60  # 2ì‹œê°„
    
    def crawl_trending_shorts(self, count: int = 30) -> List[Dict]:
        """ì‹¤ì œ YouTube Shorts í¬ë¡¤ë§ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€"""
        print(f"ğŸ¬ ì‹¤ì œ YouTube Shorts í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ)...")
        
        driver = None
        try:
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.get("https://www.youtube.com/shorts")
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì˜ìƒ ë¡œë“œ
            for _ in range(5):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
            
            # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            videos_data = self._extract_video_data(driver, count)
            
            if videos_data and len(videos_data) > 0:
                print(f"âœ… ì‹¤ì œ í¬ë¡¤ë§ ì„±ê³µ: {len(videos_data)}ê°œ ë™ì˜ìƒ")
                return videos_data
            else:
                print("âŒ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return []
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            print("ğŸ”„ ì‹¤ì œ í¬ë¡¤ë§ ì¬ì‹œë„ ì¤‘...")
            return self._retry_real_crawling(count)
        
        finally:
            if driver:
                driver.quit()
    
    def _retry_real_crawling(self, count: int) -> List[Dict]:
        """ì‹¤ì œ í¬ë¡¤ë§ ì¬ì‹œë„ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€"""
        print("ğŸ”„ ì‹¤ì œ í¬ë¡¤ë§ ì¬ì‹œë„ ì¤‘...")
        
        try:
            # ë‹¤ë¥¸ í¬ë¡¤ë§ ë°©ë²• ì‹œë„
            videos = self._get_real_trending_videos(count)
            if videos:
                print(f"âœ… ì‹¤ì œ í¬ë¡¤ë§ ì„±ê³µ: {len(videos)}ê°œ ì˜ìƒ")
                return videos
            else:
                print("âŒ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return []
        except Exception as e:
            print(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_fallback_data(self, count: int) -> List[Dict]:
        """ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš© - ìƒ˜í”Œ ë°ì´í„° ê¸ˆì§€"""
        print("âŒ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return []
    
    def _extract_video_data(self, driver, count: int) -> List[Dict]:
        """ì‹¤ì œ ì˜ìƒ ë°ì´í„° ì¶”ì¶œ"""
        videos = []
        
        try:
            # ì˜ìƒ ìš”ì†Œë“¤ ì°¾ê¸°
            video_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/shorts/']")
            
            for element in video_elements[:count]:
                try:
                    title = element.get_attribute("title") or "ì œëª© ì—†ìŒ"
                    href = element.get_attribute("href")
                    
                    if href and "/shorts/" in href:
                        video_id = href.split("/shorts/")[-1].split("?")[0]
                        
                        video_data = {
                            "title": title,
                            "category": self._categorize_video(title),
                            "views": self._get_view_count(element),
                            "engagement": "ë†’ìŒ",
                            "keywords": self._extract_keywords(title),
                            "thumbnail": "ğŸ“±",
                            "why_viral": self._analyze_viral_factors(title),
                            "video_id": video_id,
                            "youtube_url": f"https://www.youtube.com/watch?v={video_id}",
                            "shorts_url": href,
                            "is_shorts": True,
                            "region": "êµ­ë‚´",
                            "language": "í•œêµ­ì–´",
                            "trend_score": self._calculate_trend_score(title)
                        }
                        
                        videos.append(video_data)
                        
                except Exception as e:
                    continue
            
            return videos
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _categorize_video(self, title: str) -> str:
        """ì˜ìƒ ì œëª©ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['ë¶€ì—…', 'ì°½ì—…', 'ëˆ', 'ìˆ˜ìµ', 'nì¡']):
            return 'ì°½ì—…/ë¶€ì—…'
        elif any(word in title_lower for word in ['ì£¼ì‹', 'íˆ¬ì', 'ì¬í…Œí¬', 'ì½”ì¸']):
            return 'ì¬í…Œí¬/ê¸ˆìœµ'
        elif any(word in title_lower for word in ['ai', 'chatgpt', 'ì½”ë”©', 'ê°œë°œ']):
            return 'ê³¼í•™ê¸°ìˆ '
        elif any(word in title_lower for word in ['ìš´ë™', 'í—¬ìŠ¤', 'ë‹¤ì´ì–´íŠ¸']):
            return 'ìš´ë™/ê±´ê°•'
        elif any(word in title_lower for word in ['ìš”ë¦¬', 'ë ˆì‹œí”¼', 'ë¨¹ë°©']):
            return 'ìš”ë¦¬/ìŒì‹'
        else:
            return 'ê¸°íƒ€'
    
    def _get_view_count(self, element) -> str:
        """ì¡°íšŒìˆ˜ ì¶”ì¶œ"""
        try:
            view_element = element.find_element(By.CSS_SELECTOR, "[class*='view']")
            return view_element.text
        except:
            return "ì¡°íšŒìˆ˜ ì—†ìŒ"
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì£¼ìš” í‚¤ì›Œë“œ íŒ¨í„´
        patterns = {
            'ë¶€ì—…': ['ë¶€ì—…', 'nì¡', 'íˆ¬ì¡', 'ì‚¬ì´ë“œ'],
            'ëˆ': ['ëˆ', 'ìˆ˜ìµ', 'ì›”ê¸‰', 'ë²Œê¸°'],
            'ì£¼ì‹': ['ì£¼ì‹', 'íˆ¬ì', 'ì¬í…Œí¬'],
            'AI': ['ai', 'chatgpt', 'ì¸ê³µì§€ëŠ¥'],
            'ìš´ë™': ['ìš´ë™', 'í—¬ìŠ¤', 'ë‹¤ì´ì–´íŠ¸'],
            'ìš”ë¦¬': ['ìš”ë¦¬', 'ë ˆì‹œí”¼', 'ë¨¹ë°©']
        }
        
        for category, words in patterns.items():
            if any(word in title.lower() for word in words):
                keywords.append(category)
        
        return keywords[:3]
    
    def _analyze_viral_factors(self, title: str) -> str:
        """ë°”ì´ëŸ´ ìš”ì†Œ ë¶„ì„"""
        elements = []
        
        if re.search(r'\d+', title):
            elements.append("ìˆ«ì")
        if re.search(r'[!?]', title):
            elements.append("ê°ì •")
        if re.search(r'ë¹„ë²•|ê¿€íŒ|ë°©ë²•', title):
            elements.append("í•˜ìš°íˆ¬")
        
        return " + ".join(elements) if elements else "í¥ë¯¸ë¡œìš´ ì£¼ì œ"
    
    def _calculate_trend_score(self, title: str) -> int:
        """íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°"""
        score = 50  # ê¸°ë³¸ ì ìˆ˜
        
        # ì œëª© ê¸¸ì´
        if len(title) > 20:
            score += 10
        
        # ìˆ«ì í¬í•¨
        if re.search(r'\d+', title):
            score += 15
        
        # ê°ì • í‘œí˜„
        if re.search(r'[!?]', title):
            score += 10
        
        # í‚¤ì›Œë“œ ì ìˆ˜
        viral_keywords = ['ë¹„ë²•', 'ê¿€íŒ', 'ë°©ë²•', 'ê³µê°œ', 'ì§„ì§œ', 'ì ˆëŒ€']
        for keyword in viral_keywords:
            if keyword in title:
                score += 5
        
        return min(score, 100)
    
    def save_to_cache(self, data: List[Dict]):
        """ìºì‹œì— ì €ì¥"""
        cache_data = {
            "last_updated": datetime.now().isoformat(),
            "videos": data,
            "count": len(data),
            "source": "real_crawler"
        }
        
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        self.last_update = datetime.now()
        print(f"ğŸ’¾ ì‹¤ì œ ë°ì´í„° ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(data)}ê°œ ì˜ìƒ")
    
    def load_from_cache(self) -> Dict:
        """ìºì‹œì—ì„œ ë¡œë“œ"""
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
