from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict
from shorts_planner import ShortsPlannerSystem
from youtube_trends import YouTubeTrendsAnalyzer
from youtube_realtime_crawler import YouTubeRealtimeCrawler
from youtube_shorts_crawler import YouTubeShortsCrawler
from youtube_api_crawler import YouTubeAPIShortsCrawler
from youtube_ytdlp_crawler import YouTubeYTDLPCrawler
import json
from datetime import datetime, timedelta
from pathlib import Path
import subprocess
import threading

app = FastAPI(
    title="ì‡¼ì¸  ì½˜í…ì¸  ê¸°íš ì‹œìŠ¤í…œ",
    description="ì¡°íšŒìˆ˜ ë†’ì€ ì‡¼ì¸  ì½˜í…ì¸ ë¥¼ ê¸°íší•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì‡¼ì¸  í”Œë˜ë„ˆ ì´ˆê¸°í™”
planner = ShortsPlannerSystem()
youtube_analyzer = YouTubeTrendsAnalyzer()
realtime_crawler = YouTubeRealtimeCrawler()
shorts_crawler = YouTubeShortsCrawler()
api_crawler = YouTubeAPIShortsCrawler()  # YouTube Data API v3 í¬ë¡¤ëŸ¬
ytdlp_crawler = YouTubeYTDLPCrawler()  # yt-dlp í¬ë¡¤ëŸ¬ (ì‹¤ì œ ê¸‰ìƒìŠ¹ ì˜ìƒ)

# ì„œë²„ ì‹œì‘ ì‹œ ì²« í¬ë¡¤ë§ ì‹¤í–‰
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    print("ğŸš€ ì„œë²„ ì‹œì‘ - YouTube Shorts í¬ë¡¤ë§ ì‹œì‘...")
    
    import threading
    def initial_crawl():
        import time
        time.sleep(2)  # ì„œë²„ ì‹œì‘ í›„ 2ì´ˆ ëŒ€ê¸°
        try:
            print("ğŸ¬ ì¹´í…Œê³ ë¦¬ë³„ ê¸‰ìƒìŠ¹ ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘ (ì¹´í…Œê³ ë¦¬ë‹¹ 100ê°œ)...")
            
            # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë“¤
            main_categories = [
                'ì°½ì—…/ë¶€ì—…', 'ì¬í…Œí¬/ê¸ˆìœµ', 'ê³¼í•™ê¸°ìˆ ', 'ìê¸°ê³„ë°œ', 'ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤',
                'ìš”ë¦¬/ìŒì‹', 'ê²Œì„', 'ìš´ë™/ê±´ê°•', 'êµìœ¡/í•™ìŠµ', 'ìŒì•…'
            ]
            
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ 100ê°œì”© ìˆ˜ì§‘
            videos = ytdlp_crawler.get_trending_by_category(main_categories, per_category=100)
            
            if videos and len(videos) > 0:
                ytdlp_crawler.save_to_cache(videos)
                shorts_count = sum(1 for v in videos if v.get('is_shorts'))
                long_count = len(videos) - shorts_count
                korean_count = sum(1 for v in videos if v.get('language') == 'í•œêµ­ì–´')
                
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                from collections import Counter
                category_counts = Counter(v.get('category') for v in videos)
                
                print(f"\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(videos)}ê°œ")
                print(f"   ì‡¼ì¸ : {shorts_count}ê°œ | ë¡±í¼: {long_count}ê°œ")
                print(f"   í•œêµ­ì–´: {korean_count}ê°œ | ì˜ì–´: {len(videos) - korean_count}ê°œ")
                print(f"\n   ì¹´í…Œê³ ë¦¬ë³„:")
                for cat, count in category_counts.most_common():
                    print(f"   - {cat}: {count}ê°œ")
            else:
                print("âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    threading.Thread(target=initial_crawl, daemon=True).start()
    
    # 2ì‹œê°„ë§ˆë‹¤ ìë™ ì—…ë°ì´íŠ¸
    def auto_update_loop():
        import time
        while True:
            time.sleep(2 * 60 * 60)  # 2ì‹œê°„
            try:
                print(f"ğŸ”„ [{datetime.now().strftime('%H:%M:%S')}] 2ì‹œê°„ ì£¼ê¸° ìë™ í¬ë¡¤ë§ ì‹œì‘...")
                
                main_categories = [
                    'ì°½ì—…/ë¶€ì—…', 'ì¬í…Œí¬/ê¸ˆìœµ', 'ê³¼í•™ê¸°ìˆ ', 'ìê¸°ê³„ë°œ', 'ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤',
                    'ìš”ë¦¬/ìŒì‹', 'ê²Œì„', 'ìš´ë™/ê±´ê°•', 'êµìœ¡/í•™ìŠµ', 'ìŒì•…'
                ]
                
                videos = ytdlp_crawler.get_trending_by_category(main_categories, per_category=100)
                if videos and len(videos) > 0:
                    ytdlp_crawler.save_to_cache(videos)
                    print(f"âœ… ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(videos)}ê°œ")
                else:
                    print("âš ï¸ ìë™ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            except Exception as e:
                print(f"âŒ ìë™ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    threading.Thread(target=auto_update_loop, daemon=True).start()
    print("âœ… yt-dlp ìë™ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘ (2ì‹œê°„ë§ˆë‹¤)")

# Request/Response Models
class ContentPlanRequest(BaseModel):
    topic: str
    content_type: str = "Actionable"
    target_audience: Optional[str] = None
    user_story: Optional[Dict] = None

class ContentPlanResponse(BaseModel):
    plan: Dict
    generated_at: str

class NicheAnalysisRequest(BaseModel):
    topic: str
    target_audience: Optional[str] = None

class HookGenerationRequest(BaseModel):
    topic: str
    count: int = 10

class SavedPlan(BaseModel):
    id: str
    topic: str
    content_type: str
    plan: Dict
    created_at: str

# Storage file for saved plans
STORAGE_FILE = Path("../data/saved_plans.json")

def load_saved_plans() -> List[dict]:
    """ì €ì¥ëœ ê¸°íšì„œ ë¡œë“œ"""
    if STORAGE_FILE.exists():
        with open(STORAGE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_plan_to_file(plan: dict):
    """ê¸°íšì„œ ì €ì¥"""
    saved = load_saved_plans()
    saved.append(plan)
    
    STORAGE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(STORAGE_FILE, 'w', encoding='utf-8') as f:
        json.dump(saved, f, ensure_ascii=False, indent=2)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "ì‡¼ì¸  ì½˜í…ì¸  ê¸°íš ì‹œìŠ¤í…œ - ì¡°íšŒìˆ˜ ë†’ì€ ì½˜í…ì¸ ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.post("/api/create-plan", response_model=ContentPlanResponse)
async def create_content_plan(request: ContentPlanRequest):
    """ì½˜í…ì¸  ê¸°íšì„œ ìƒì„±"""
    try:
        plan = planner.create_content_plan(
            topic=request.topic,
            content_type=request.content_type,
            target_audience=request.target_audience or "",
            user_story=request.user_story
        )
        
        # ìƒì„± ì‹œê°„ ì¶”ê°€
        plan['ìƒì„±_ì¼ì‹œ'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        return ContentPlanResponse(
            plan=plan,
            generated_at=plan['ìƒì„±_ì¼ì‹œ']
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê¸°íšì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/api/analyze-niche")
async def analyze_niche(request: NicheAnalysisRequest):
    """ë‹ˆì¹˜ ë¶„ì„"""
    try:
        analysis = planner.analyze_niche(
            topic=request.topic,
            target_audience=request.target_audience or ""
        )
        return {"analysis": analysis}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹ˆì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/generate-hooks")
async def generate_hooks(request: HookGenerationRequest):
    """í›… ì•„ì´ë””ì–´ ìƒì„±"""
    try:
        hooks = planner.generate_hooks(
            topic=request.topic,
            count=request.count
        )
        return {"hooks": hooks}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í›… ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/content-types")
async def get_content_types():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì½˜í…ì¸  íƒ€ì…"""
    content_types = [
        {
            "value": "Actionable",
            "label": "ì‹¤í–‰ ê°€ëŠ¥í•œ ê°€ì´ë“œ",
            "description": "ë‹¨ê³„ë³„ ë°©ë²• ì œì‹œ",
            "best_for": "íŠœí† ë¦¬ì–¼, í•˜ìš°íˆ¬"
        },
        {
            "value": "Motivational",
            "label": "ë™ê¸°ë¶€ì—¬ ìŠ¤í† ë¦¬",
            "description": "ì˜ê°ì„ ì£¼ëŠ” ì´ì•¼ê¸°",
            "best_for": "Before/After, ì„±ê³µ ìŠ¤í† ë¦¬"
        },
        {
            "value": "Analytical",
            "label": "ë¶„ì„ ë° í•´ë¶€",
            "description": "ì‹¬ì¸µ ë¶„ì„",
            "best_for": "ë¦¬ë·°, ë¹„êµ"
        },
        {
            "value": "Contrarian",
            "label": "ë°˜ëŒ€ ì˜ê²¬",
            "description": "ì¼ë°˜ ìƒì‹ì— ë„ì „",
            "best_for": "ë…¼ë€, í™”ì œì„±"
        },
        {
            "value": "Observation",
            "label": "ê´€ì°° ë° ì¸ì‚¬ì´íŠ¸",
            "description": "í¥ë¯¸ë¡œìš´ ë°œê²¬",
            "best_for": "íŠ¸ë Œë“œ, í˜„ìƒ ë¶„ì„"
        },
        {
            "value": "X vs. Y",
            "label": "ë¹„êµ ë¶„ì„",
            "description": "ë‘ ê°€ì§€ ë¹„êµ",
            "best_for": "ë¹„êµ, ëŒ€ê²°"
        },
        {
            "value": "Present/Future",
            "label": "í˜„ì¬ì™€ ë¯¸ë˜",
            "description": "íŠ¸ë Œë“œ ì˜ˆì¸¡",
            "best_for": "ì „ë§, ì˜ˆì¸¡"
        },
        {
            "value": "Listicle",
            "label": "ëª©ë¡í˜•",
            "description": "ë¦¬ìŠ¤íŠ¸ í˜•ì‹",
            "best_for": "TOP 10, ì¶”ì²œ"
        }
    ]
    return {"content_types": content_types}

@app.get("/api/trending-topics")
async def get_trending_topics():
    """íŠ¸ë Œë”© ì£¼ì œ ì¶”ì²œ"""
    try:
        topics = planner.get_trending_topics()
        return {"trending_topics": topics}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŠ¸ë Œë”© ì£¼ì œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/suggest-hashtags")
async def suggest_hashtags(request: dict):
    """í•´ì‹œíƒœê·¸ ì œì•ˆ"""
    try:
        topic = request.get("topic", "")
        category = request.get("category", "")
        
        hashtags = planner.suggest_hashtags(topic, category)
        return {"hashtags": hashtags}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•´ì‹œíƒœê·¸ ì œì•ˆ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/save-plan")
async def save_plan(content: dict):
    """ê¸°íšì„œ ì €ì¥"""
    try:
        plan_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_item = {
            "id": plan_id,
            "topic": content.get("topic"),
            "content_type": content.get("content_type"),
            "plan": content.get("plan"),
            "created_at": datetime.now().isoformat()
        }
        
        save_plan_to_file(saved_item)
        
        return {"message": "ê¸°íšì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤", "id": plan_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê¸°íšì„œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/saved-plans")
async def get_saved_plans():
    """ì €ì¥ëœ ê¸°íšì„œ ëª©ë¡"""
    try:
        saved = load_saved_plans()
        return {"saved_plans": saved, "count": len(saved)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê¸°íšì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.delete("/api/saved-plans/{plan_id}")
async def delete_saved_plan(plan_id: str):
    """ê¸°íšì„œ ì‚­ì œ"""
    try:
        saved = load_saved_plans()
        saved = [item for item in saved if item.get('id') != plan_id]
        
        with open(STORAGE_FILE, 'w', encoding='utf-8') as f:
            json.dump(saved, f, ensure_ascii=False, indent=2)
        
        return {"message": "ê¸°íšì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê¸°íšì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/optimization-checklist")
async def get_optimization_checklist():
    """ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸"""
    checklist = planner.system_data.get("ì‡¼ì¸ _ìµœì í™”", {})
    return {
        "checklist": checklist.get("ì¡°íšŒìˆ˜_ìµœì í™”_ì²´í¬ë¦¬ìŠ¤íŠ¸", []),
        "viral_elements": checklist.get("ë°”ì´ëŸ´_ìš”ì†Œ", []),
        "platforms": checklist.get("í”Œë«í¼ë³„_ì „ëµ", {})
    }

@app.get("/api/youtube/trending")
async def get_youtube_trending(
    count: int = 50,
    category: Optional[str] = None,
    region: Optional[str] = None,
    language: Optional[str] = None,
    min_trend_score: Optional[int] = None,
    sort_by: str = "trend_score",
    force_refresh: bool = False,
    video_type: Optional[str] = None,  # "ì‡¼ì¸ " ë˜ëŠ” "ë¡±í¼" í•„í„°
    time_filter: Optional[str] = None  # "today", "week", "month", "all"
):
    """YouTube ê¸‰ìƒìŠ¹ ë™ì˜ìƒ (ì‡¼ì¸ +ë¡±í¼, í•„í„°ë§ ì§€ì›)"""
    try:
        # force_refreshê°€ Trueë©´ ì¦‰ì‹œ í¬ë¡¤ë§
        cache_file = Path("../data/youtube_shorts_cache.json")
        should_refresh = force_refresh
        
        if force_refresh:
            print("ğŸ”„ ì‚¬ìš©ì ìš”ì²­: ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë°ì´í„° ì¦‰ì‹œ í¬ë¡¤ë§...")
            import threading
            def force_crawl():
                try:
                    main_categories = [
                        'ì°½ì—…/ë¶€ì—…', 'ì¬í…Œí¬/ê¸ˆìœµ', 'ê³¼í•™ê¸°ìˆ ', 'ìê¸°ê³„ë°œ', 'ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤',
                        'ìš”ë¦¬/ìŒì‹', 'ê²Œì„', 'ìš´ë™/ê±´ê°•', 'êµìœ¡/í•™ìŠµ', 'ìŒì•…'
                    ]
                    videos = ytdlp_crawler.get_trending_by_category(main_categories, per_category=100)
                    if videos:
                        ytdlp_crawler.save_to_cache(videos)
                        print(f"âœ… ì¦‰ì‹œ í¬ë¡¤ë§ ì™„ë£Œ: {len(videos)}ê°œ (ì¹´í…Œê³ ë¦¬ë³„ 100ê°œì”©)")
                except Exception as e:
                    print(f"âŒ ì¦‰ì‹œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            threading.Thread(target=force_crawl, daemon=True).start()
        
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            # ìºì‹œê°€ 1ì‹œê°„ ì´ìƒ ì˜¤ë˜ëœ ê²½ìš°ì—ë§Œ ìƒˆë¡œê³ ì¹¨
            if cache.get('last_updated'):
                last_updated = datetime.fromisoformat(cache['last_updated'].replace('Z', '+00:00'))
                time_diff = (datetime.now() - last_updated).total_seconds()
                if time_diff > 3600:  # 1ì‹œê°„
                    should_refresh = True
                    print(f"ğŸ”„ ìºì‹œê°€ {int(time_diff/3600)}ì‹œê°„ ì „ ë°ì´í„° - ìƒˆë¡œê³ ì¹¨")
        
        if should_refresh or not cache_file.exists():
            print("ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìµœì‹  ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹œì‘ (ê¸°ì¡´ ë°ì´í„° ë¨¼ì € ë°˜í™˜)
            import threading
            def background_crawl():
                try:
                    videos = shorts_crawler.crawl_shorts_trending(200)
                    shorts_crawler.save_to_cache(videos)
                    print("âœ… ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ")
                except Exception as e:
                    print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
            threading.Thread(target=background_crawl, daemon=True).start()
            
            # ê¸°ì¡´ ìºì‹œê°€ ìˆìœ¼ë©´ ë¨¼ì € ë°˜í™˜
            if cache and cache.get('videos'):
                print("ğŸ“Š ê¸°ì¡´ ë°ì´í„° ë¨¼ì € ë°˜í™˜, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì—…ë°ì´íŠ¸ ì¤‘...")
            else:
                # ìºì‹œê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ í¬ë¡¤ë§
                videos = shorts_crawler.crawl_shorts_trending(200)
                shorts_crawler.save_to_cache(videos)
                cache = {"videos": videos, "last_updated": datetime.now().isoformat()}
        
        if cache and cache.get('videos'):
            videos = cache['videos']
            
            # í•„í„°ë§ ì ìš©
            filtered_videos = _apply_filters(videos, category, region, language, min_trend_score, video_type, time_filter)
            
            print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: {len(filtered_videos)}ê°œ (ì „ì²´ {len(videos)}ê°œ)")
            if category:
                print(f"   - ì¹´í…Œê³ ë¦¬: {category}")
            if region:
                print(f"   - ì§€ì—­: {region}")
            if time_filter:
                print(f"   - ê¸°ê°„: {time_filter}")
            
            # ì •ë ¬
            sorted_videos = _sort_videos(filtered_videos, sort_by)
            
            # ê°œìˆ˜ ì œí•œ
            final_videos = sorted_videos[:count]
            
            return {
                "trending_videos": final_videos,
                "count": len(final_videos),
                "total_count": len(filtered_videos),
                "filters_applied": {
                    "category": category,
                    "region": region,
                    "language": language,
                    "min_trend_score": min_trend_score,
                    "sort_by": sort_by,
                    "video_type": video_type,
                    "time_filter": time_filter
                },
                "last_updated": cache.get('last_updated'),
                "source": "shorts_cache",
                "auto_refreshed": should_refresh
            }
        
        # ìºì‹œ ì—†ìœ¼ë©´ ì¦‰ì‹œ Shorts í¬ë¡¤ë§
        print("Shorts ìºì‹œ ì—†ìŒ - ì¦‰ì‹œ í¬ë¡¤ë§ ì‹¤í–‰")
        videos = shorts_crawler.crawl_shorts_trending(count)
        shorts_crawler.save_to_cache(videos)
        
        return {
            "trending_videos": videos,
            "count": len(videos),
            "last_updated": datetime.now().isoformat(),
            "source": "fresh_shorts_crawl"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Shorts íŠ¸ë Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

def _apply_filters(videos: List[Dict], category: Optional[str], region: Optional[str], 
                  language: Optional[str], min_trend_score: Optional[int], video_type: Optional[str],
                  time_filter: Optional[str]) -> List[Dict]:
    """ë¹„ë””ì˜¤ í•„í„°ë§ (ê°œì„ ëœ ë²„ì „)"""
    filtered = videos.copy()
    
    print(f"ğŸ” í•„í„°ë§ ì‹œì‘: ì´ {len(filtered)}ê°œ ì˜ìƒ")
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if category:
        before = len(filtered)
        filtered = [v for v in filtered if v.get('category', '').strip() == category.strip()]
        print(f"   ì¹´í…Œê³ ë¦¬ '{category}' í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    # ì§€ì—­ í•„í„°
    if region:
        before = len(filtered)
        filtered = [v for v in filtered if v.get('region', '').strip() == region.strip()]
        print(f"   ì§€ì—­ '{region}' í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    # ì–¸ì–´ í•„í„°
    if language:
        before = len(filtered)
        filtered = [v for v in filtered if v.get('language', '').strip() == language.strip()]
        print(f"   ì–¸ì–´ '{language}' í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    # íŠ¸ë Œë“œ ì ìˆ˜ í•„í„°
    if min_trend_score:
        before = len(filtered)
        filtered = [v for v in filtered if v.get('trend_score', 0) >= min_trend_score]
        print(f"   íŠ¸ë Œë“œ ì ìˆ˜ {min_trend_score}+ í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    # ì‡¼ì¸ /ë¡±í¼ í•„í„°
    if video_type:
        before = len(filtered)
        filtered = [v for v in filtered if v.get('video_type', '').strip() == video_type.strip()]
        print(f"   ì˜ìƒ íƒ€ì… '{video_type}' í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    # ê¸°ê°„ í•„í„°
    if time_filter and time_filter != "all":
        before = len(filtered)
        now = datetime.now()
        
        if time_filter == "today":
            # ì˜¤ëŠ˜ (ìµœê·¼ 24ì‹œê°„)
            cutoff = now - timedelta(days=1)
        elif time_filter == "week":
            # ì´ë²ˆ ì£¼ (ìµœê·¼ 7ì¼)
            cutoff = now - timedelta(days=7)
        elif time_filter == "month":
            # ì´ë²ˆ ë‹¬ (ìµœê·¼ 30ì¼)
            cutoff = now - timedelta(days=30)
        else:
            cutoff = None
        
        if cutoff:
            def is_recent(video):
                crawled_at = video.get('crawled_at')
                if not crawled_at:
                    return False
                try:
                    crawled_time = datetime.fromisoformat(crawled_at.replace('Z', '+00:00'))
                    # timezone-awareì¸ ê²½ìš° naiveë¡œ ë³€í™˜
                    if crawled_time.tzinfo is not None:
                        crawled_time = crawled_time.replace(tzinfo=None)
                    return crawled_time >= cutoff
                except (ValueError, AttributeError):
                    return False
            
            filtered = [v for v in filtered if is_recent(v)]
            print(f"   ê¸°ê°„ '{time_filter}' í•„í„°: {before}ê°œ â†’ {len(filtered)}ê°œ")
    
    print(f"âœ… ìµœì¢… í•„í„°ë§ ê²°ê³¼: {len(filtered)}ê°œ")
    return filtered

def _sort_videos(videos: List[Dict], sort_by: str) -> List[Dict]:
    """ë¹„ë””ì˜¤ ì •ë ¬"""
    if sort_by == "trend_score":
        return sorted(videos, key=lambda x: x.get('trend_score', 0), reverse=True)
    elif sort_by == "views":
        # ì¡°íšŒìˆ˜ ì •ë ¬ (K, M ì²˜ë¦¬)
        def parse_views(views_str):
            if 'M' in views_str:
                return float(views_str.replace('M', '')) * 1000000
            elif 'K' in views_str:
                return float(views_str.replace('K', '')) * 1000
            else:
                try:
                    return float(views_str.replace(',', ''))
                except:
                    return 0
        return sorted(videos, key=lambda x: parse_views(x.get('views', '0')), reverse=True)
    elif sort_by == "crawled_at":
        return sorted(videos, key=lambda x: x.get('crawled_at', ''), reverse=True)
    else:
        return videos

@app.post("/api/youtube/refresh")
async def refresh_youtube_trending():
    """ìµœì‹  ë°ì´í„°ë¡œ ê°•ì œ ì—…ë°ì´íŠ¸ - ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ í›„ ì‚¬ìš©"""
    try:
        print("ğŸ”„ ìµœì‹  ë°ì´í„° í™•ì¸ ì¤‘...")
        
        cache_file = Path("../data/youtube_shorts_cache.json")
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            # ìºì‹œê°€ ìµœê·¼ 10ë¶„ ì´ë‚´ë©´ ìµœì‹  ë°ì´í„°
            if cache.get('last_updated'):
                last_updated = datetime.fromisoformat(cache['last_updated'].replace('Z', '+00:00'))
                time_diff = (datetime.now() - last_updated).total_seconds()
                if time_diff < 600:  # 10ë¶„ ì´ë‚´
                    print("âœ… ì´ë¯¸ ìµœì‹  ë°ì´í„°ì…ë‹ˆë‹¤")
                    return {
                        "message": "ì´ë¯¸ ìµœì‹  ë°ì´í„°ì…ë‹ˆë‹¤",
                        "last_updated": cache['last_updated'],
                        "count": len(cache.get('videos', [])),
                        "source": "cached_data",
                        "status": "already_fresh"
                    }
        
        # ìµœì‹  ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ê°•ì œ í¬ë¡¤ë§
        print("ğŸ”„ ìµœì‹  ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
        videos = shorts_crawler.crawl_shorts_trending(200)
        shorts_crawler.save_to_cache(videos)
        
        print(f"âœ… ìµœì‹  ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(videos)}ê°œ ë™ì˜ìƒ")
        
        return {
            "message": "ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ",
            "last_updated": datetime.now().isoformat(),
            "count": len(videos),
            "source": "fresh_crawl",
            "status": "success"
        }
    except Exception as e:
        print(f"âŒ ìµœì‹  ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/youtube/category-keywords/{category}")
async def get_category_keywords(category: str):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ í•« í‚¤ì›Œë“œ ë¶„ì„"""
    try:
        # ìºì‹œì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì˜ìƒë“¤ ê°€ì ¸ì˜¤ê¸°
        cache_file = Path("../data/youtube_shorts_cache.json")
        if not cache_file.exists():
            raise HTTPException(status_code=404, detail="ìºì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        category_videos = [v for v in cache_data.get('videos', []) if v.get('category') == category]
        
        if not category_videos:
            return {
                "category": category,
                "keywords": [],
                "message": f"{category} ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
        all_keywords = []
        for video in category_videos:
            all_keywords.extend(video.get('keywords', []))
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_keywords = []
        for keyword, count in keyword_counts.most_common(20):
            if keyword and len(keyword) > 1:  # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 1ê¸€ì í‚¤ì›Œë“œ ì œì™¸
                top_keywords.append({
                    "keyword": keyword,
                    "frequency": count,
                    "percentage": round((count / len(category_videos)) * 100, 1)
                })
        
        return {
            "category": category,
            "total_videos": len(category_videos),
            "keywords": top_keywords,
            "last_updated": cache_data.get('last_updated', datetime.now().isoformat())
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/youtube/filter-options")
async def get_filter_options():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í•„í„° ì˜µì…˜ ì œê³µ"""
    try:
        cache_file = Path("../data/youtube_shorts_cache.json")
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            if cache and cache.get('videos'):
                videos = cache['videos']
                
                # ì‹¤ì œ ë°ì´í„°ì—ì„œ ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬
                found_categories = list(set(v.get('category', '') for v in videos if v.get('category')))
                
                # ì „ì²´ YouTube ì¹´í…Œê³ ë¦¬ ëª©ë¡
                all_categories = [
                    "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜", "ìë™ì°¨/ì°¨ëŸ‰", "ìŒì•…", "ë°˜ë ¤ë™ë¬¼/ë™ë¬¼", "ìŠ¤í¬ì¸ ",
                    "ì—¬í–‰/ì´ë²¤íŠ¸", "ê²Œì„", "ì¸ë¬¼/ë¸”ë¡œê·¸", "ì½”ë¯¸ë””", "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                    "ë‰´ìŠ¤/ì •ì¹˜", "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼", "êµìœ¡", "ê³¼í•™ê¸°ìˆ ", "ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™",
                    "ìš”ë¦¬/ìŒì‹", "ìê¸°ê³„ë°œ", "ì¬í…Œí¬/ê¸ˆìœµ", "ì°½ì—…/ë¶€ì—…", "ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤", "ì¼ë°˜"
                ]
                
                # ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬ì™€ ì „ì²´ ì¹´í…Œê³ ë¦¬ ê²°í•© (ì¤‘ë³µ ì œê±°)
                categories = sorted(list(set(found_categories + all_categories)))
                
                # ì§€ì—­ ì˜µì…˜
                regions = list(set(v.get('region', '') for v in videos if v.get('region')))
                
                # ì–¸ì–´ ì˜µì…˜ (í•­ìƒ í•œêµ­ì–´, ì˜ì–´ í¬í•¨)
                languages_from_data = list(set(v.get('language', '') for v in videos if v.get('language')))
                languages = sorted(list(set(languages_from_data + ["í•œêµ­ì–´", "ì˜ì–´"])))
                
                # ì •ë ¬ ì˜µì…˜
                sort_options = [
                    {"value": "trend_score", "label": "íŠ¸ë Œë“œ ì ìˆ˜"},
                    {"value": "views", "label": "ì¡°íšŒìˆ˜"},
                    {"value": "crawled_at", "label": "ìµœì‹ ìˆœ"}
                ]
                
                # ê¸°ê°„ í•„í„° ì˜µì…˜
                time_filter_options = [
                    {"value": "all", "label": "ì „ì²´"},
                    {"value": "today", "label": "ì˜¤ëŠ˜ (24ì‹œê°„)"},
                    {"value": "week", "label": "ì´ë²ˆ ì£¼ (7ì¼)"},
                    {"value": "month", "label": "ì´ë²ˆ ë‹¬ (30ì¼)"}
                ]
                
                # ì˜ìƒ íƒ€ì… ì˜µì…˜
                video_type_options = [
                    {"value": "", "label": "ì „ì²´"},
                    {"value": "ì‡¼ì¸ ", "label": "ì‡¼ì¸ "},
                    {"value": "ë¡±í¼", "label": "ë¡±í¼"}
                ]
                
                return {
                    "categories": sorted(categories),
                    "regions": sorted(regions),
                    "languages": sorted(languages),
                    "sort_options": sort_options,
                    "time_filter_options": time_filter_options,
                    "video_type_options": video_type_options,
                    "trend_score_range": {
                        "min": 1,
                        "max": 100,
                        "default": 50
                    }
                }
        
        # ê¸°ë³¸ ì˜µì…˜ ë°˜í™˜ (ì˜ì–´ í•­ìƒ í¬í•¨)
        return {
            "categories": [
                "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜",
                "ìë™ì°¨/ì°¨ëŸ‰",
                "ìŒì•…",
                "ë°˜ë ¤ë™ë¬¼/ë™ë¬¼",
                "ìŠ¤í¬ì¸ ",
                "ì—¬í–‰/ì´ë²¤íŠ¸",
                "ê²Œì„",
                "ì¸ë¬¼/ë¸”ë¡œê·¸",
                "ì½”ë¯¸ë””",
                "ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "ë‰´ìŠ¤/ì •ì¹˜",
                "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼",
                "êµìœ¡",
                "ê³¼í•™ê¸°ìˆ ",
                "ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™",
                "ìš”ë¦¬/ìŒì‹",
                "ìê¸°ê³„ë°œ",
                "ì¬í…Œí¬/ê¸ˆìœµ",
                "ì°½ì—…/ë¶€ì—…",
                "ë§ˆì¼€íŒ…/ë¹„ì¦ˆë‹ˆìŠ¤",
                "ì¼ë°˜"
            ],
            "regions": ["êµ­ë‚´", "í•´ì™¸"],
            "languages": ["í•œêµ­ì–´", "ì˜ì–´"],  # í•­ìƒ ì–‘ìª½ ì–¸ì–´ í‘œì‹œ
            "sort_options": [
                {"value": "trend_score", "label": "íŠ¸ë Œë“œ ì ìˆ˜"},
                {"value": "views", "label": "ì¡°íšŒìˆ˜"},
                {"value": "crawled_at", "label": "ìµœì‹ ìˆœ"}
            ],
            "trend_score_range": {
                "min": 1,
                "max": 100,
                "default": 50
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í•„í„° ì˜µì…˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/youtube/analyze-keywords")
async def analyze_keywords(request: dict):
    """ê¸‰ìƒìŠ¹ ë™ì˜ìƒì—ì„œ í‚¤ì›Œë“œ ë¶„ì„"""
    try:
        videos = request.get("videos", [])
        if not videos:
            videos = youtube_analyzer.get_trending_videos(20)
        
        analysis = youtube_analyzer.extract_keywords_from_videos(videos)
        return {"keyword_analysis": analysis}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/youtube/content-ideas")
async def get_content_ideas(request: dict):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì½˜í…ì¸  ì•„ì´ë””ì–´"""
    try:
        keyword = request.get("keyword", "")
        videos = youtube_analyzer.get_trending_videos(20)
        
        ideas = youtube_analyzer.suggest_content_ideas(keyword, videos)
        return {"content_ideas": ideas}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì•„ì´ë””ì–´ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/youtube/posting-times")
async def get_posting_times():
    """ìµœì  ì—…ë¡œë“œ ì‹œê°„"""
    try:
        times = youtube_analyzer.get_optimal_posting_times()
        return {"posting_times": times}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì‹œê°„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "system_loaded": bool(planner.system_data),
        "youtube_analyzer_loaded": youtube_analyzer is not None
    }

# ìŠ¤ì¼€ì¤„ëŸ¬ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
scheduler_process = None

@app.post("/api/start-scheduler")
async def start_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    global scheduler_process
    
    if scheduler_process and scheduler_process.poll() is None:
        return {"status": "already_running", "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤"}
    
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
        scheduler_process = subprocess.Popen(
            ['python', 'scheduler.py'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        return {
            "status": "success", 
            "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
            "pid": scheduler_process.pid
        }
    except Exception as e:
        return {"status": "error", "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {str(e)}"}

@app.post("/api/stop-scheduler")
async def stop_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    global scheduler_process
    
    if not scheduler_process or scheduler_process.poll() is not None:
        return {"status": "not_running", "message": "ì‹¤í–‰ ì¤‘ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì—†ìŠµë‹ˆë‹¤"}
    
    try:
        scheduler_process.terminate()
        scheduler_process.wait(timeout=10)
        return {"status": "success", "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        return {"status": "error", "message": f"ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ì‹¤íŒ¨: {str(e)}"}

@app.get("/api/scheduler-status")
async def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸"""
    global scheduler_process
    
    if scheduler_process and scheduler_process.poll() is None:
        return {
            "status": "running",
            "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤",
            "pid": scheduler_process.pid
        }
    else:
        return {
            "status": "stopped",
            "message": "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        }

@app.post("/api/trigger-crawling")
async def trigger_crawling():
    """ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        # ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤í–‰
        result1 = subprocess.run(['python', 'youtube_realtime_crawler.py'], 
                               capture_output=True, text=True, timeout=300)
        
        # ì‡¼ì¸  í¬ë¡¤ë§ ì‹¤í–‰
        result2 = subprocess.run(['python', 'youtube_shorts_crawler.py'], 
                               capture_output=True, text=True, timeout=300)
        
        return {
            "status": "success",
            "message": "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "realtime_result": result1.returncode == 0,
            "realtime_output": result1.stdout,
            "shorts_result": result2.returncode == 0,
            "shorts_output": result2.stdout
        }
    except Exception as e:
        return {"status": "error", "message": f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
